{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFyQRz6T8KfbDn297BFrH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejpat98/Textbook-Summarisation/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4En3CxHwJN"
      },
      "source": [
        "(W.I.P) Implementation of sequence to sequence model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXUYzhe9HuRM"
      },
      "source": [
        "!pip install tensorflow-gpu\r\n",
        "!pip install tensorflow-addons\r\n",
        "!pip install tensorflow-datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J10h4kclIhB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HguLnusJ0m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2413f6-9e54-4bbf-a468-d011fdda1784"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_addons as tfa\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import pandas as pd\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krWyTx10JCCY"
      },
      "source": [
        "Prepairing arXiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9_fjjeSn9t"
      },
      "source": [
        "sp_data = pd.read_json('/content/123.json', lines=True)\r\n",
        "print(sp_data.info())\r\n",
        "print('----------------------------------------------------------------')\r\n",
        "\r\n",
        "sp_cleaned_data = pd.DataFrame({\r\n",
        "    'article_id': [],\r\n",
        "    'article_text': [],\r\n",
        "    'abstract_text': [],\r\n",
        "    'labels': [],\r\n",
        "    'section_names': [],\r\n",
        "    'sections': []\r\n",
        "})\r\n",
        "data = {'article_id': [],\r\n",
        "        'article_text': [],\r\n",
        "        'abstract_text': [],\r\n",
        "        'labels': [],\r\n",
        "        'section_names': [],\r\n",
        "        'sections': []}\r\n",
        "usable = True\r\n",
        "counter = 0\r\n",
        "\r\n",
        "def KeepPaper(index):\r\n",
        "    data['article_id'].append(sp_data['article_id'][index])\r\n",
        "    data['article_text'].append(sp_data['article_text'][index])\r\n",
        "    data['abstract_text'].append(sp_data['abstract_text'][index])\r\n",
        "    data['labels'].append(sp_data['labels'][index])\r\n",
        "    data['section_names'].append(sp_data['section_names'][index])\r\n",
        "    data['sections'].append(sp_data['sections'][index])\r\n",
        "    return\r\n",
        "\r\n",
        "#Where 'i' is the article body and 'j' is a sentence\r\n",
        "for i, i2 in enumerate(sp_data['article_id']):\r\n",
        "    print(\"Checking article id : \" + str(i2) + \" ... \")\r\n",
        "    for j in sp_data['article_text'][i]:\r\n",
        "        if j.find(\"@xmath\") != -1 or j.find(\"@xcite\") != -1:\r\n",
        "            # Found a sentence that contains either maths or a citation. --> not usable\r\n",
        "            usable = False\r\n",
        "            break\r\n",
        "\r\n",
        "    if usable:\r\n",
        "        # No maths or citations found --> copy paper to sp_cleaned_data\r\n",
        "        print(\"*** Found useful paper *** \" + \"article_id: \" + str(i2))\r\n",
        "        counter += 1\r\n",
        "        KeepPaper(i)\r\n",
        "    usable = True\r\n",
        "\r\n",
        "print(\"Total useful papers found: \" + str(counter) + \" out of \" + str(len(sp_data['article_id'])))\r\n",
        "\r\n",
        "papers = pd.Series(data=data,\r\n",
        "                   index=['article_id', 'article_text', 'abstract_text', 'labels', 'section_names', 'sections'])\r\n",
        "\r\n",
        "sp_cleaned_data = sp_cleaned_data.append(papers, ignore_index=True)\r\n",
        "\r\n",
        "sp_cleaned_data.to_json('/content/sp_cleaned.json')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}