{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPe/hG+QXgb7aqHQTbjtCo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejpat98/Textbook-Summarisation/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV64K4w3_7pO"
      },
      "source": [
        "**Preprocessing textbook dataset:**\r\n",
        "*   Download textbooks (OP / CC licences)\r\n",
        "*   Convert PDF into raw text data\r\n",
        "*   Split textbooks into chapters\r\n",
        "*   Put into a labeled dataset format\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_Hzs0zrYI-n"
      },
      "source": [
        "Notes:\r\n",
        "https://stackoverflow.com/questions/60754884/python-ocr-pytesseract-for-pdf \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdlUeDIM_6SB"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9EFdnbC6VPl"
      },
      "source": [
        "!apt-get install poppler-utils\r\n",
        "!apt-get install tesseract-ocr\r\n",
        "!pip install PyPDF2\r\n",
        "!pip install pdf2image\r\n",
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pLtlp0ARQwT"
      },
      "source": [
        "!add-apt-repository ppa:alex-p/tesseract-ocr\r\n",
        "!apt-get update\r\n",
        "!apt install libtesseract-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eFiLXMBSxqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f49416e-7c1f-497e-ee1d-bcf03bfba64e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 14:34:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXjqbl9GdKua"
      },
      "source": [
        "import GPUtil\r\n",
        "GPUs = GPUtil.getGPUs()\r\n",
        "for i, gpu in enumerate(GPUs):\r\n",
        "  print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdlMPZEt4As9"
      },
      "source": [
        "import os\r\n",
        "from PIL import Image\r\n",
        "from pdf2image import convert_from_path\r\n",
        "import pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGZpwrAM2a15"
      },
      "source": [
        "Convert PDF into Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rs7XTygFJBc"
      },
      "source": [
        "%cd '/content/drive/MyDrive/University/Computer Science - Level 3/FYP/Open and CC Textbooks (PDF)/27/'\r\n",
        "path='/content/drive/MyDrive/University/Computer Science - Level 3/FYP/Open and CC Textbooks (PDF)/27/images/'\r\n",
        "fileName = '27.pdf'\r\n",
        "doc = convert_from_path(fileName, 300, output_folder=path, paths_only=True, fmt=\"jpeg\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-gCTGlT2gOv"
      },
      "source": [
        "Convert Images into Text using PyTesseract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfuFZeJcuD3t"
      },
      "source": [
        "for i in range(23,28):\r\n",
        "  path = '/content/drive/MyDrive/University/Computer Science - Level 3/FYP/Open and CC Textbooks (PDF)/'+str(i)+'/images/'\r\n",
        "  ImageNames = os.listdir(\"/content/drive/MyDrive/University/Computer Science - Level 3/FYP/Open and CC Textbooks (PDF)/\"+str(i)+\"/images/\")\r\n",
        "  print(len(ImageNames))\r\n",
        "  PrefList= ImageNames[0].strip().split('-')[:-1]\r\n",
        "  pref = '-'.join(PrefList)\r\n",
        "\r\n",
        "  for i in range(1,len(ImageNames)):\r\n",
        "    with open(path+'**output.txt', 'a') as f:\r\n",
        "      txt = pytesseract.image_to_string(Image.open(path+str(pref)+'-'+str(i).zfill(len(str(len(ImageNames))))+'.jpg'))\r\n",
        "      f.write(\"*****Page: \" + str(i) +\"*****\")\r\n",
        "      f.write(\"\\n\")\r\n",
        "      f.write(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR3-6k2peobT"
      },
      "source": [
        "1. Clean textbook dataset\r\n",
        "2. Create extractive summaries(simple ones) for the textbook dataset\r\n",
        "3. Augment with whole [science_papers](https://www.tensorflow.org/datasets/catalog/scientific_papers) dataset from TF\r\n",
        "4. Train a basic seq2seq model on 80% + science_papers datasets\r\n",
        "5. Test model using 20% of textbook dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSFT0rRHYkCF"
      },
      "source": [
        "import json\r\n",
        "counter = 1\r\n",
        "offset = 9\r\n",
        "Book = {\r\n",
        "    'chapter_name': [],\r\n",
        "    'chapter_text': [],\r\n",
        "    'cleaned_chapter_text': []\r\n",
        "}\r\n",
        "chapter = [\r\n",
        "    [\"\"],\r\n",
        "    [\"EOF\", 153]]\r\n",
        "i = 0\r\n",
        "chapter_text = []\r\n",
        "with open('C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/Textbook dataset/__output (7).txt', encoding='utf-8', buffering=1) as f:\r\n",
        "    for line in f:\r\n",
        "        if \"*****Page: \" in line:\r\n",
        "            #Found start of new page\r\n",
        "            pageNum = line.split(\" \")[1][:-6]\r\n",
        "\r\n",
        "        if int(pageNum) >= (chapter[i][1] + offset) and int(pageNum) < (chapter[i+1][1] + offset):\r\n",
        "            chapter_text.append(str(line))\r\n",
        "        elif int(pageNum) == (chapter[i+1][1] + offset):\r\n",
        "            Book['chapter_name'].append(chapter[i][0])\r\n",
        "            Book['chapter_text'].append(chapter_text[:])\r\n",
        "            chapter_text.clear()\r\n",
        "            i +=1\r\n",
        "\r\n",
        "Book['chapter_name'].append(chapter[i][0])\r\n",
        "Book['chapter_text'].append(chapter_text[:])\r\n",
        "chapter_text.clear()\r\n",
        "\r\n",
        "dump = json.dumps(Book)\r\n",
        "with open('C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/Textbook dataset/TB json/book_7.txt', 'x') as f2:\r\n",
        "    f2.write(dump)\r\n",
        "    f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulaoc363CLyJ"
      },
      "source": [
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "# 1. Remove bad chapters\r\n",
        "# 2. Remove empty string, page number headers, punctuation, all numbers, URLs and special characters/symbols (including \" \\n \")\r\n",
        "# 3. Put all strings into a spell checker\r\n",
        "# 4. ONLY then do stemming and other NLP optimisations\r\n",
        "Book = {\r\n",
        "    'chapter_name': [],\r\n",
        "    'chapter_text': [],\r\n",
        "    'cleaned_chapter_text': []\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def checkResult(title):\r\n",
        "    index = 0\r\n",
        "    print(title + \": \")\r\n",
        "    for m, n in enumerate(Book['cleaned_chapter_text']):\r\n",
        "        if m == index:\r\n",
        "            print(str(m) + \": \", n)\r\n",
        "    return\r\n",
        "\r\n",
        "\r\n",
        "def decontracte(phrase):\r\n",
        "    # specific\r\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\r\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\r\n",
        "    # general\r\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\r\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\r\n",
        "    return phrase\r\n",
        "\r\n",
        "\r\n",
        "with open('C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/Textbook dataset/TB json/book_7.txt', buffering=1) as f:\r\n",
        "    for line in f:\r\n",
        "        data = json.loads(line)\r\n",
        "        for i, j in enumerate(data['chapter_name']):\r\n",
        "            Book['chapter_name'].append(data['chapter_name'][i])\r\n",
        "            Book['chapter_text'].append(data['chapter_text'][i])\r\n",
        "            Book['cleaned_chapter_text'].append(data['chapter_text'][i])\r\n",
        "\r\n",
        "checkResult(\"Original\")\r\n",
        "\r\n",
        "# Remove new lines \"\\n\"\r\n",
        "for i, j in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k, l in enumerate(j):\r\n",
        "        Book['cleaned_chapter_text'][i][k] = Book['cleaned_chapter_text'][i][k].replace(\" \\n \", \"\")\r\n",
        "        Book['cleaned_chapter_text'][i][k] = Book['cleaned_chapter_text'][i][k].replace(\" \\n\", \"\")\r\n",
        "        Book['cleaned_chapter_text'][i][k] = Book['cleaned_chapter_text'][i][k].replace(\"\\n \", \"\")\r\n",
        "        Book['cleaned_chapter_text'][i][k] = Book['cleaned_chapter_text'][i][k].replace(\"\\n\", \"\")\r\n",
        "checkResult(\"Removed newline\")\r\n",
        "\r\n",
        "# Remove empty string\r\n",
        "for x in range(2):\r\n",
        "    for i2, j2 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "        for k2, l2 in enumerate(j2):\r\n",
        "            if l2 == \"\":\r\n",
        "                del Book['cleaned_chapter_text'][i2][k2]\r\n",
        "checkResult(\"Removed empty strings\")\r\n",
        "\r\n",
        "# Remove page tags\r\n",
        "for i3, j3 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k3, l3 in enumerate(j3):\r\n",
        "        if l3.find(\"*****Page: \") != -1:\r\n",
        "            del Book['cleaned_chapter_text'][i3][k3]\r\n",
        "checkResult(\"Removed page tags\")\r\n",
        "\r\n",
        "# Remove URL\r\n",
        "for i4, j4 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k4, l4 in enumerate(j4):\r\n",
        "        URL = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', l4)\r\n",
        "        if URL:\r\n",
        "            Book['cleaned_chapter_text'][i4][k4] = Book['cleaned_chapter_text'][i4][k4].replace(URL[0], \"@xURL\")\r\n",
        "checkResult(\"Removed URLs\")\r\n",
        "\r\n",
        "# Remove numbers\r\n",
        "for i5, j5 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k5, l5 in enumerate(j5):\r\n",
        "        no_nums = re.sub(r'\\d+', '', l5)\r\n",
        "        Book['cleaned_chapter_text'][i5][k5] = no_nums\r\n",
        "checkResult(\"Removed numbers\")\r\n",
        "\r\n",
        "# Remove contractions\r\n",
        "for i6, j6 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k6, l6 in enumerate(j6):\r\n",
        "        no_cont = decontracte(str(l6))\r\n",
        "        Book['cleaned_chapter_text'][i6][k6] = no_cont\r\n",
        "checkResult(\"Removed contractions\")\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "for i7, j7 in enumerate(Book['cleaned_chapter_text']):\r\n",
        "    for k7, l7 in enumerate(j7):\r\n",
        "        no_punc = re.sub(r'[^\\w]', ' ', l7)\r\n",
        "        Book['cleaned_chapter_text'][i7][k7] = no_punc\r\n",
        "checkResult(\"Remove punctuation\")\r\n",
        "\r\n",
        "dump = json.dumps(Book)\r\n",
        "with open('C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/Textbook dataset/TB json/with cleaned/book_cleaned_9999.txt', 'x') as f2:\r\n",
        "    f2.write(dump)\r\n",
        "    f2.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTFHsSDMCjjU"
      },
      "source": [
        "Preprocessing research papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8hyHwI3CipV"
      },
      "source": [
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "papers = {\r\n",
        "    'article_id': [],\r\n",
        "    'article_text': [],\r\n",
        "    'cleaned_article_text': [],\r\n",
        "    'abstract_text': []}\r\n",
        "\r\n",
        "\r\n",
        "def checkResult(title):\r\n",
        "    index = 0\r\n",
        "    print(title + \": \")\r\n",
        "    print(papers['cleaned_article_text'][0][0])\r\n",
        "    return\r\n",
        "\r\n",
        "\r\n",
        "def decontracte(phrase):\r\n",
        "    # specific\r\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\r\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\r\n",
        "    # general\r\n",
        "    phrase = re.sub(r\"non-\", \"not \", phrase)\r\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\r\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\r\n",
        "    return phrase\r\n",
        "\r\n",
        "\r\n",
        "with open(\"C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/arxiv-dataset/arxiv-dataset/train_split/train_split_21_3037.txt\", encoding='utf-8', buffering=1) as f:\r\n",
        "    for line in f:\r\n",
        "        data = json.loads(line)\r\n",
        "        papers['article_id'].append(data['article_id'])\r\n",
        "        papers['article_text'].append(data['article_text'])\r\n",
        "        papers['cleaned_article_text'].append(data['article_text'])\r\n",
        "        papers['abstract_text'].append(data['abstract_text'])\r\n",
        "\r\n",
        "checkResult(\"Original\")\r\n",
        "\r\n",
        "# Remove new lines \"\\n\"\r\n",
        "for i, j in enumerate(papers['article_id']):\r\n",
        "    for k, l in enumerate(papers['cleaned_article_text'][i]):\r\n",
        "        for m, n in enumerate(l):\r\n",
        "            papers['cleaned_article_text'][i][k][m] = papers['cleaned_article_text'][i][k][m].replace(\" \\n \", \"\")\r\n",
        "            papers['cleaned_article_text'][i][k][m] = papers['cleaned_article_text'][i][k][m].replace(\" \\n\", \"\")\r\n",
        "            papers['cleaned_article_text'][i][k][m] = papers['cleaned_article_text'][i][k][m].replace(\"\\n \", \"\")\r\n",
        "            papers['cleaned_article_text'][i][k][m] = papers['cleaned_article_text'][i][k][m].replace(\"\\n\", \"\")\r\n",
        "\r\n",
        "checkResult(\"Removed newline\")\r\n",
        "\r\n",
        "# Remove empty string\r\n",
        "for x in range(2):\r\n",
        "    for i2, j2 in enumerate(papers['article_id']):\r\n",
        "        for k2, l2 in enumerate(papers['cleaned_article_text'][i2]):\r\n",
        "            for m2, n2 in enumerate(l2):\r\n",
        "                if n2 == \"\":\r\n",
        "                    del papers['cleaned_article_text'][i2][k2][m2]\r\n",
        "checkResult(\"Removed empty strings\")\r\n",
        "\r\n",
        "# Remove numbers\r\n",
        "for i4, j4 in enumerate(papers['article_id']):\r\n",
        "    for k4, l4 in enumerate(papers['cleaned_article_text'][i4]):\r\n",
        "        for m4, n4 in enumerate(l4):\r\n",
        "            no_nums = re.sub(r'\\d+', '', n4)\r\n",
        "            papers['cleaned_article_text'][i4][k4][m4] = no_nums\r\n",
        "checkResult(\"Removed numbers\")\r\n",
        "\r\n",
        "# Remove contractions\r\n",
        "for i5, j5 in enumerate(papers['article_id']):\r\n",
        "    for k5, l5 in enumerate(papers['cleaned_article_text'][i5]):\r\n",
        "        for m5, n5 in enumerate(l5):\r\n",
        "            no_cont = decontracte(str(n5))\r\n",
        "            papers['cleaned_article_text'][i5][k5][m5] = no_cont\r\n",
        "checkResult(\"Removed contractions\")\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "for i6, j6 in enumerate(papers['article_id']):\r\n",
        "    for k6, l6 in enumerate(papers['cleaned_article_text'][i6]):\r\n",
        "        for m6, n6 in enumerate(l6):\r\n",
        "            no_punc = re.sub(r'[^\\w]', ' ', n6)\r\n",
        "            papers['cleaned_article_text'][i6][k6][m6] = no_punc\r\n",
        "checkResult(\"Remove punctuation\")\r\n",
        "\r\n",
        "dump = json.dumps(papers)\r\n",
        "with open('C:/Users/Tejash/Desktop/Comp Sci/Year 3/FYP/Textbook dataset/TB json/cleaned papers/paper_cleaned_1.txt',\r\n",
        "          'x') as f2:\r\n",
        "    f2.write(dump)\r\n",
        "    f2.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}